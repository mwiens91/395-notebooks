{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.integrate as integrate\n",
    "from scipy.special import fresnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS 395 - week 2\n",
    "\n",
    "**Matt Wiens - #301294492**\n",
    "\n",
    "This notebook will be organized similarly to the lab script, with major headings corresponding to the headings on the lab script.\n",
    "\n",
    "*The TA's name (Ignacio) will be shortened to \"IC\" whenever used.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical error homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Truncation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncation error is the error involved in approximating an infinite sum by a finite sum. For example, the truncation error involved in approximating $e^x$ to the first three terms of its Taylor series is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left| \\, e^x - \\left(1 + x + \\frac{x^2}{2}\\right) \\, \\right|\n",
    "    .\n",
    "\\end{equation}\n",
    "\n",
    "For numerical integration, truncation error comes in to play because we are approximating an integral (an infinite sum) by a finite sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rounding error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any algorithm which produces a result, rounding error is the difference between producing that result using exact arithmetic and producing the result using finite-precision, rounded arithmetic (floating point arithmetic). On a computer, a number called \"machine epsilon\" gives you the upper bound of the relative error that occurs in floating point arithmetic due to rounding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demonstrating floating point arithmetic error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(7 / 3 - 4 / 3 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above in exact arithmetic should be $0$. But due to error in floating point arithmetic it is instead a non-zero small number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical integration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this section will be to investigate how we can approximate integrals according to different \"rules\".    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple rule\n",
    "\n",
    "Consider some function $f$ defined on an interval $[a, b]$. A simple rule to approximate the integral of $f$ over this interval is to take $N + 1$ evenly spaced points along the interval $\\{x_1, x_2, \\ldots, x_{N + 1}\\}$ and then add up the area of the rectangles induced by these points (see the picture in the lab script). Note that we always have $x_1 = a$ and $x_N = b$. This gives us the following approximation:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\int_a^b f(x) dx \\approx \\sum_{i = 1}^N f(x_i) h\n",
    "    ,\n",
    "\\end{equation}\n",
    "\n",
    "where $h$ is the width of each rectangle given by\n",
    "\n",
    "\\begin{equation}\n",
    "    h = \\frac{b - a}{N}\n",
    "    .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_left_reimann(y: np.ndarray, h: float) -> float:\n",
    "    \"\"\"Approximates an integral using the left Reimann sum.\"\"\"\n",
    "    return np.sum(y * h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will approximate the integral of $\\sin(x)$ over $[0, \\frac{\\pi}{2}]$ using logarithmically spaced values of $N$. We will plot the absolute error as a function of the bin width $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a number of N values and the corresponding h values\n",
    "num_ns = 50\n",
    "\n",
    "# We need to be a little careful since we want the N values to\n",
    "# be logarithmically space, but we also require that each N is an integer.\n",
    "n_vals = np.round(np.logspace(1, 6, num_ns)).astype(int)\n",
    "h_vals = np.pi / (2 * n_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we plot the absolute errors, note that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\int_0^{\\frac{\\pi}{2}} \\sin(x) dx = 1\n",
    "    .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each N value calculate the absolute error\n",
    "errors = np.zeros(num_ns)\n",
    "\n",
    "for idx, n in enumerate(n_vals):\n",
    "    xs = np.linspace(0, np.pi / 2, n)\n",
    "    ys = np.sin(xs)\n",
    "\n",
    "    errors[idx] = abs(1 - integrate_left_reimann(ys, np.pi / (2 * n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(h_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$h$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a linear relationship in the log-log plot. This means that the error E is related to $h$ through some relationship of the form\n",
    "\n",
    "\\begin{equation}\n",
    "    E = A h^\\alpha\n",
    "    .\n",
    "\\end{equation}\n",
    "\n",
    "This is because a linear relationship in log-log can be expressed as\n",
    "\n",
    "\\begin{align}\n",
    "    &\\log E = \\alpha \\log h + \\log A \\\\\n",
    "    &\\iff \\log E = \\log \\left( A h^\\alpha \\right) \\\\\n",
    "    &\\iff E = A h^\\alpha\n",
    "    .\n",
    "\\end{align}\n",
    "\n",
    "By inspection, I would guess that $\\alpha = 1$ and $A = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to fit the log values we obtained to a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the linear coefficients for the log-log relationship\n",
    "coeffs = np.polyfit(x=np.log(h_vals), y=np.log(errors), deg=1)\n",
    "\n",
    "# Translate to A and h\n",
    "print(\"A = %.2f\" % np.exp(coeffs[1]))\n",
    "print(\"alpha = %.2f\" % coeffs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My guess for $\\alpha$ was correct, but the value for $A$ I guessed was not quite right (it's difficult to make out on the above plot what $A$ should be with any precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the coefficients we calculated, we can estimate that to get an error less than or equal to $10^{-8}$ we would need a width of \n",
    "\n",
    "\\begin{align}\n",
    "    h &= \\left( \\frac{E}{A} \\right)^{\\frac{1}{\\alpha}} \\\\\n",
    "      &\\approx \\frac{10^{-8}}{0.14} \\\\\n",
    "      &\\approx 8 \\cdot 10^{-8}\n",
    "      ,\n",
    "\\end{align}\n",
    "\n",
    "which means we need at least $20916033$ slices using this method. Note that the size of $h$ depends linearly on the error $E$ in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting calculations for above text block\n",
    "needed_h = (1e-8 / np.exp(coeffs[1])) ** (1 / coeffs[0])\n",
    "needed_n = np.pi / 2 / needed_h\n",
    "\n",
    "print(\"approx h required: %e\" % needed_h)\n",
    "print(\"approx N required: %f\" % needed_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trapezoid rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the lab script we are shown a picture of how the trapezoid rule works. Given $x$ values $\\{x_0, x_1, x_2, x_3\\}$ and corresponding function values $\\{y_0, y_1, y_2, y_3\\}$. There are many ways to compute the sum of the trapezoids: for each trapezoid I will add the the rectangle induced the height of the right endpoint together with the remaining half rectangle. This gives us the formula for the sum $S$ as\n",
    "\n",
    "\\begin{align}\n",
    "    S &= \\sum_{i = 1}^3\n",
    "        \\left(\n",
    "            y_i h\n",
    "            + \\frac{1}{2} \\left( y_{i - 1} - y_i \\right) h\n",
    "        \\right) \\\\\n",
    "      &= \\frac{h}{2} \\sum_{i = 1}^3\n",
    "        \\left(\n",
    "            y_{i - 1} + y_i\n",
    "        \\right)\n",
    "\\end{align}\n",
    "\n",
    "where $h = x_i - x_{i - 1}$ (assumed constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the trapezoid rule, we will repeat the investigation we performed when using the left Riemann approximation. We will keep the same number of $N$ values to test and the corresponding $h$ values we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each N value calculate the absolute error\n",
    "errors = np.zeros(num_ns)\n",
    "\n",
    "for idx, n in enumerate(n_vals):\n",
    "    xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "    ys = np.sin(xs)\n",
    "\n",
    "    errors[idx] = abs(1 - integrate.trapz(y=ys, dx=np.pi / (2 * n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(h_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$h$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the log-log relationship is still linear. However, for the same values of $h$ the error in the trapezoidal rule is much lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate the $A$ and $\\alpha$ parameters and estimate what $h$ and $N$ we would need to keep the absolute error below $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the linear coefficients for the log-log relationship\n",
    "coeffs = np.polyfit(x=np.log(h_vals), y=np.log(errors), deg=1)\n",
    "\n",
    "# Translate to A and h\n",
    "print(\"A = %.2f\" % np.exp(coeffs[1]))\n",
    "print(\"alpha = %.2f\" % coeffs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependence of $h$ on the error $E$ is now squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the greatest h/ least N we can use?\n",
    "needed_h = (1e-8 / np.exp(coeffs[1])) ** (1 / coeffs[0])\n",
    "needed_n = np.pi / 2 / needed_h\n",
    "\n",
    "print(\"approx h required: %e\" % needed_h)\n",
    "print(\"approx N required: %f\" % needed_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare with the left Riemann approximation, we need far fewer steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpson's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for Simpson's method we need an *even* number of slices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll plot the errors for different values of $h$. Note that we need to be careful to make sure $N$ is even here, so we'll adjust each $N$ value so that this holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to be a little careful since we want the N values to\n",
    "# be logarithmically space, but we also require that each N is an integer.\n",
    "simps_n_vals = n_vals + n_vals % 2\n",
    "simps_h_vals = np.pi / (2 * n_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each N value calculate the absolute error\n",
    "errors = np.zeros(num_ns)\n",
    "\n",
    "for idx, n in enumerate(simps_n_vals):\n",
    "    xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "    ys = np.sin(xs)\n",
    "\n",
    "    errors[idx] = abs(1 - integrate.simps(y=ys, dx=np.pi / (2 * n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(simps_h_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$h$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly see that Simpson's method outperforms both methods we looked at before. We quickly reach the limits of machine precision as we increase $N$. The dependence is still linear, although due to the limits of machine precision, we cannot show the full relationship we would get with exact arithmetic here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If wanted to fit the linear relationship we need to look at the values of $N$ that lead to $h \\approx 10^{-3}$ and higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last h value index to keep\n",
    "last_h_idx = 20\n",
    "\n",
    "# Find the linear coefficients for the log-log relationship\n",
    "coeffs = np.polyfit(\n",
    "    x=np.log(simps_h_vals[: last_h_idx + 1]), y=np.log(errors[: last_h_idx + 1]), deg=1\n",
    ")\n",
    "\n",
    "# Translate to A and h\n",
    "print(\"A = %.2f\" % np.exp(coeffs[1]))\n",
    "print(\"alpha = %.2f\" % coeffs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of dependence of $h$ on $E$ is to the fourth power. Now we can again determine what is the greatest $h$/least $N$ we need to keep the error under $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the greatest h/ least N we can use?\n",
    "needed_h = (1e-8 / np.exp(coeffs[1])) ** (1 / coeffs[0])\n",
    "needed_n = np.pi / 2 / needed_h\n",
    "\n",
    "print(\"approx h required: %e\" % needed_h)\n",
    "print(\"approx N required: %f\" % needed_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the $\\alpha$ value or by looking that the greatest $N$ required, Simpson's method is about $100$ times as efficient as the trapezoid rule and far, far more efficient as the left Riemann sum approximation for this integral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing comparison\n",
    "\n",
    "Let's compare how long each of these methods take. While we know Simpson's method uses fewer steps, for example, each step could take longer, so up to now, we still don't know which method is \"best\". We'll now time each method using the $N$ value for each method which gives error below $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N values\n",
    "test_n_vals = n_vals + n_vals % 2\n",
    "test_h_vals = np.pi / (2 * n_vals)\n",
    "\n",
    "# Left Riemann\n",
    "n = 20916033\n",
    "xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "ys = np.sin(xs)\n",
    "h = np.pi / (2 * n)\n",
    "\n",
    "start = timer()\n",
    "\n",
    "integrate_left_reimann(y=ys, h=h)\n",
    "\n",
    "lriem_time = timer() - start\n",
    "\n",
    "# Trapezoid\n",
    "n = 4535\n",
    "xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "ys = np.sin(xs)\n",
    "h = np.pi / (2 * n)\n",
    "\n",
    "start = timer()\n",
    "\n",
    "integrate.simps(y=ys, dx=h)\n",
    "\n",
    "trapz_time = timer() - start\n",
    "\n",
    "# Simpson's\n",
    "n = 43\n",
    "xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "ys = np.sin(xs)\n",
    "h = np.pi / (2 * n)\n",
    "\n",
    "start = timer()\n",
    "\n",
    "integrate.simps(y=ys, dx=h)\n",
    "\n",
    "simps_time = timer() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the times\n",
    "print(\"Left Riemann:\\t%.5f\" % lriem_time)\n",
    "print(\"Trapezoid:\\t%.5f\" % trapz_time)\n",
    "print(\"Simpson's:\\t%.5f\" % simps_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least for this integral, Simpson's method is by bar the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1 homework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fresnel integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fresnel integral is given by the following function:\n",
    "\n",
    "\\begin{equation}\n",
    "    C(x) = \\int_0^x \\cos \\left( \\frac{\\pi}{2} x^2 \\right) dx\n",
    "    .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first plot the integrand on the range $[0, 7.1]$.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 7.1, 500)\n",
    "ys = np.cos(np.pi / 2 * xs ** 2)\n",
    "\n",
    "# Plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.plot(xs, ys)\n",
    "\n",
    "ax.set_xlabel(r\"$x$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SciPy, we can evaluate $C$ at $x = 7.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, c_val = fresnel(7.1)\n",
    "\n",
    "print(c_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Simpson's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used the numerical integration methods we discussed above, however, we would need a very large number of slices $N$. This is because, as we can see in the above plot, the integrand of the Fresnel function changes rapidly, which means that any approximation per slice, especially as $x$ gets large, is going to be bad. In this case, it might make more sense to have fewer slices near the beginning of the interval (when the integrand doesn't change as rapidly), and more slices as we progress through the interval, since the integrand is changing more and more with increasing $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find what $h$ and $N$ we need using Simpson's method to keep the error less then $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each N value calculate the absolute error\n",
    "simps_n_vals = n_vals + n_vals % 2\n",
    "simps_h_vals = 7.1 / simps_n_vals\n",
    "\n",
    "errors = np.zeros(num_ns)\n",
    "\n",
    "for idx, n in enumerate(simps_n_vals):\n",
    "    xs = np.linspace(0, 7.1, n + 1)\n",
    "    ys = np.cos(np.pi / 2 * xs ** 2)\n",
    "\n",
    "    errors[idx] = abs(c_val - integrate.simps(y=ys, dx=simps_h_vals[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(simps_h_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$h$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll determine the coefficients of the log-log relationship using the $h$ values that appear linear on the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which h indices we'll use\n",
    "first_h_idx = 8\n",
    "last_h_idx = 35\n",
    "\n",
    "# Find the linear coefficients for the log-log relationship\n",
    "coeffs = np.polyfit(\n",
    "    x=np.log(simps_h_vals[first_h_idx : last_h_idx + 1]),\n",
    "    y=np.log(errors[first_h_idx : last_h_idx + 1]),\n",
    "    deg=1,\n",
    ")\n",
    "\n",
    "# Translate to A and h\n",
    "print(\"A = %.2f\" % np.exp(coeffs[1]))\n",
    "print(\"alpha = %.2f\" % coeffs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that determining these coefficients is more sensitive to the data points chosen compared to the cases we looked at earlier. Nevertheless, the coefficients are relatively stable at the above values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the greatest h/ least N we can use?\n",
    "needed_h = (1e-8 / np.exp(coeffs[1])) ** (1 / coeffs[0])\n",
    "needed_n = 7.1 / needed_h\n",
    "\n",
    "print(\"approx h required: %e\" % needed_h)\n",
    "print(\"approx N required: %f\" % needed_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of steps $N$ is much higher than what we needed for the simpler integral we first looked at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using non-uniformly spaces points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine what $N$ we need using SciPy's implementation of fixed-order Gaussian quadrature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The integrand\n",
    "integrand = lambda x: np.cos(np.pi / 2 * x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the errors. We'll use fewer N values here because it can\n",
    "# take quite awhile using the N values we've been using so far.\n",
    "quad_num_ns = 50\n",
    "quad_n_vals = np.round(np.logspace(1, 3.5, quad_num_ns)).astype(int)\n",
    "errors = np.zeros(quad_num_ns)\n",
    "\n",
    "for idx, n in enumerate(quad_n_vals):\n",
    "    errors[idx] = abs(c_val - integrate.fixed_quad(func=integrand, a=0, b=7.1, n=n)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors against N\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(quad_n_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$N$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the above plot, it looks like we can use around $N \\approx 45$ to reach an absolute error below $10^{-8}$. Let's verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error with N = 90\n",
    "this_error = abs(c_val - integrate.fixed_quad(func=integrand, a=0, b=7.1, n=45)[0])\n",
    "\n",
    "print(\"error with N=%d: %e\" % (45, this_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy also provides another function which lets us specify the tolerance specifically. Let's try it with a tolerance of $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"value:\\t%e\\nerror:\\t%e\"\n",
    "    % integrate.quadrature(func=integrand, a=0, b=7.1, tol=1e-8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating improper integrals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a SciPy quadrature function to evaluate improper integrals like\n",
    "\n",
    "\\begin{equation}\n",
    "    \\int_{-\\infty}^\\infty \\exp \\left(- \\frac{x^2}{2} \\right) dx = \\sqrt{2 \\pi}\n",
    "    .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"value:\\t%e\\nerror:\\t%e\"\n",
    "    % integrate.quad(func=lambda x: np.exp(-(x ** 2) / 2), a=-np.inf, b=np.inf)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll revisit the Fresnel integral but this time using Monte Carlo integration. We'll keep using the $N$ values in our `n_vals` array; note however that the interpretation behind $N$ is different: before $N$ was the number of slices and now it is the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors\n",
    "errors = np.zeros(num_ns)\n",
    "\n",
    "for idx, n in enumerate(n_vals):\n",
    "    xs = np.random.uniform(0, 7.1, n)\n",
    "    ys = np.cos(np.pi / 2 * xs ** 2)\n",
    "\n",
    "    est = 7.1 * np.sum(ys) / n\n",
    "\n",
    "    errors[idx] = abs(c_val - est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors against N\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(n_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$N$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For estimating the Fresnal integral, Monte Carlo seems like a poor choice compared to the other options we've investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can approximate a derivative of a function $f$ numerically with\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{df}{dx} \\approx \\frac{f(x + h) - f(x)}{h}\n",
    "    ,\n",
    "\\end{equation}\n",
    "\n",
    "where $h$ is a finite number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate the derivative of $f(x) = x^{0.8}$ evaluated at $x = 1$ using different values of $h$. Note that\n",
    "\n",
    "\\begin{equation}\n",
    "    f^\\prime(1) = 0.8\n",
    "    .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function and where we're going to evaluate its derivative\n",
    "f = lambda x: x ** 0.8\n",
    "eval_pt = 1\n",
    "exact_val = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the function and its derivative quickly before trying numerical differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range to plot over\n",
    "xs = np.linspace(0.1, 2, 500)\n",
    "\n",
    "# Plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.plot(xs, f(xs))\n",
    "plt.plot(xs, 0.8 / xs ** 0.2)\n",
    "\n",
    "# Legend and axis label\n",
    "ax.legend([r\"$f$\", r\"$f^\\prime$\"])\n",
    "ax.set_xlabel(r\"$x$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative is smooth and doesn't change much, so I don't anticipate any problems with numerical differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to approximate the derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The h values to use\n",
    "h_vals = 10 ** np.arange(-15, 0).astype(float)\n",
    "num_hs = len(h_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the errors\n",
    "errors = np.zeros(num_hs)\n",
    "\n",
    "for idx, h in enumerate(h_vals):\n",
    "    est = (f(eval_pt + h) - f(eval_pt)) / h\n",
    "    errors[idx] = abs(est - exact_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(h_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$h$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is at first an unintuitive result: after a certain point as $h$ gets smaller, the approximation gets worse. This is due to limits in finite-precision arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to come up with a better approximation to the derivative using Taylor expansions. Using\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x \\pm h) = f(x) \\pm h f^\\prime(x) + \\mathcal{O}(h^2)\n",
    "    ,\n",
    "\\end{equation}\n",
    "\n",
    "we have\n",
    "\n",
    "\\begin{align}\n",
    "    f(x + h) - f(x - h)\n",
    "        &=\n",
    "           \\left(\n",
    "            f(x) + h f^\\prime(x) + \\mathcal{O}(h^2)\n",
    "           \\right)\n",
    "           -\n",
    "           \\left(\n",
    "            f(x) - h f^\\prime(x) + \\mathcal{O}(h^2)\n",
    "           \\right)\n",
    "        \\\\\n",
    "        &= 2 h f^\\prime(x) + \\mathcal{O}(h^2)\n",
    "        ,\n",
    "\\end{align}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{f(x + h) - f(x - h)}{2 h} = f^\\prime(x) + \\mathcal{O}(h^2)\n",
    "    .\n",
    "\\end{equation}\n",
    "\n",
    "(Note that the remaining terms are actually $\\mathcal{O}(h^3)$ which you can see if you derive the expression using more terms in the Taylor expansions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this actually better? Let's investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the errors\n",
    "errors = np.zeros(num_hs)\n",
    "\n",
    "for idx, h in enumerate(h_vals):\n",
    "    est = (f(eval_pt + h) - f(eval_pt - h)) / (2 * h)\n",
    "    errors[idx] = abs(est - exact_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the errors\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(h_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$h$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a bit better, if we make sure to take $h$ large enough that we don't bring in too much error from finite-precision arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to ordinary differential equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euler method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a ball which starts at a height of $1$m and with initial velocity $5$ m/s. The exact trajectory $y(t)$ of this ball is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    y(t) = -\\frac{g}{2} t^2 + 5 t\n",
    "    .\n",
    "\\end{equation}\n",
    "\n",
    "Let's plot this trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravity constant\n",
    "g = 9.81\n",
    "\n",
    "# Plot trajectory\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "ts_exact = np.linspace(0, 1, 500)\n",
    "ys_exact = -g / 2 * ts_exact ** 2 + 5 * ts_exact + 1\n",
    "\n",
    "plt.plot(ts_exact, ys_exact)\n",
    "\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$y$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's approximate the trajectory using Euler's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time step\n",
    "h = 0.01\n",
    "num_steps = int(1 / h) + 1\n",
    "\n",
    "# Set up arrays for position and velocity\n",
    "ts = np.arange(0, 1 + h, step=h)\n",
    "ys = np.zeros(num_steps)\n",
    "vs = np.zeros(num_steps)\n",
    "\n",
    "# Record initial velocity and position\n",
    "ys[0] = 1\n",
    "vs[0] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Euler's method\n",
    "for i in range(1, num_steps):\n",
    "    vs[i] = vs[i - 1] - h * g\n",
    "    ys[i] = ys[i - 1] + h * vs[i - 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the approximated trajectory against the exact trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.plot(ts_exact, ys_exact)\n",
    "plt.plot(ts, ys, \"o-\")\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "\n",
    "ax.legend([\"exact\", \"Euler\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agreement is surprisingly good. However, we see that at later times the approximation gets worse; this is due to accumulation of error that occurs in Euler's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use SciPy's `solve_ivp` function to solve the free fall problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = integrate.solve_ivp(fun=lambda t, y: [y[1], -g], t_span=(0, 1), y0=[1, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this alongside the exact trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.plot(ts_exact, ys_exact)\n",
    "plt.plot(res.t, res.y[0], \"o-\")\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "\n",
    "ax.legend([\"exact\", \"ivp_solve\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few time points here but also extremely accurate. Let's use more time points, using the same `ts` array we used for Euler's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute\n",
    "res = integrate.solve_ivp(\n",
    "    fun=lambda t, y: [y[1], -g], t_span=(0, 1), y0=[1, 5], t_eval=ts\n",
    ")\n",
    "\n",
    "# Plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.plot(ts_exact, ys_exact)\n",
    "plt.plot(res.t, res.y[0], \"o-\")\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "\n",
    "ax.legend([\"exact\", \"ivp_solve\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this plot with Euler's method above. Both used the same number of time points, but in terms of accuracy `ivp_solve` performed much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the equation of motion for a damped harmonic oscillator with forcing\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{d^2 x}{dt^2} + 2 \\gamma \\omega_0 \\frac{dx}{dt} + \\omega_0^2 x = f\n",
    "\\end{equation}\n",
    "\n",
    "(see the lab script for what the constants $\\gamma, \\omega_0, f$ represent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've done before, let's write this as a system of two first-order differential equations:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{dv}{dt}\n",
    "        &= f - 2 \\gamma \\omega_0 v - \\omega_0^2 x \\\\\n",
    "    \\frac{dx}{dt}\n",
    "        &= v\n",
    "    .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that returns these derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_dho(\n",
    "    t: float, x: float, v: float, gamma: float, omega0: float, f: float\n",
    ") -> list:\n",
    "    \"\"\"Derivatives for damped harmonic oscillator.\"\"\"\n",
    "    return [v, f - 2 * gamma * omega0 * v - omega0 ** 2 * x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will solve this system using `solve_ivp`, trying out different values of $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Times to evaluate at\n",
    "h = 0.01\n",
    "ts = np.arange(0, 10 + h, step=h)\n",
    "\n",
    "t_span = (0, 10)\n",
    "\n",
    "# Try with different values of gamma\n",
    "gammas = [0, 0.25, 0.5, 1, 2]\n",
    "\n",
    "# Initial conditions\n",
    "ics = [0, 0]\n",
    "\n",
    "# Other params\n",
    "f_val = 1\n",
    "omega0 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the results\n",
    "results = [\n",
    "    integrate.solve_ivp(\n",
    "        fun=lambda t, y: fn_dho(\n",
    "            t=t, x=y[0], v=y[1], gamma=gamma, omega0=omega0, f=f_val\n",
    "        ),\n",
    "        t_span=t_span,\n",
    "        y0=ics,\n",
    "        t_eval=ts,\n",
    "    )\n",
    "    for gamma in gammas\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "for res in results:\n",
    "    plt.plot(res.t, res.y[0])\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$x$\")\n",
    "\n",
    "ax.legend([r\"$\\gamma$ = %.2f\" % gamma for gamma in gammas]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $\\gamma = 0$, obviously there is no damping. Let's have a look at the non-zero values of $\\gamma$ in a separate plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "# This plot is going to be crowded so let's use thinner lines\n",
    "line_width = 0.7\n",
    "\n",
    "# Plot dotted line at x = 0.25\n",
    "plt.plot(np.linspace(0, 10, 250), 0.25 * np.ones(250), \"--\", lw=line_width)\n",
    "\n",
    "# Print the non-zero gamma results\n",
    "for res in results[1:]:\n",
    "    plt.plot(res.t, res.y[0], lw=line_width)\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$x$\")\n",
    "\n",
    "ax.legend([\"x = 0.25\"] + [r\"$\\gamma$ = %.2f\" % gamma for gamma in gammas[1:]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line $x = 0.25$ is plotted in blue, which makes it easier to see the behaviour of our results. We see that for $\\gamma < 1$ our solutions are underdamped, the solution for $\\gamma = 1$ turns out to be critically damped, and for $\\gamma > 1$ our solutions are overdamped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look at a similar system as we saw before, except this time with time dependent forcing:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{d^2 x}{dt^2} + 2 \\gamma \\omega \\frac{dx}{dt} + \\omega^2 x = f \\cos \\omega t\n",
    "    .\n",
    "\\end{equation}\n",
    "\n",
    "We can write this as the following system of ODEs:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{dv}{dt}\n",
    "        &= f \\cos \\omega t - 2 \\gamma \\omega v - \\omega^2 x \\\\\n",
    "    \\frac{dx}{dt}\n",
    "        &= v\n",
    "    .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_dho_forced(\n",
    "    t: float, x: float, v: float, gamma: float, omega: float, f: float\n",
    ") -> list:\n",
    "    \"\"\"Derivatives for damped harmonic oscillator.\"\"\"\n",
    "    return [v, f * np.cos(omega * t) - 2 * gamma * omega * v - omega0 ** 2 * x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Varying $\\omega$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing from the application we just looked at, let's fix $\\gamma = 0.2$ (so we will have underdamped behaviour). Let's instead vary $\\omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Times to evaluate at\n",
    "start_t = 0\n",
    "end_t = 30\n",
    "\n",
    "ts = np.linspace(start_t, end_t, 1000)\n",
    "\n",
    "# Try with different values of omega\n",
    "omega_0 = 2\n",
    "omega_scaling_factors = [0.5, 0.95, 1.45]\n",
    "\n",
    "omegas = omega_0 * np.array(omega_scaling_factors)\n",
    "\n",
    "# Initial conditions\n",
    "ics = [0, 0] \n",
    "\n",
    "# Other params\n",
    "f_val = 1\n",
    "gamma = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the results\n",
    "results = [\n",
    "    integrate.solve_ivp(\n",
    "        fun=lambda t, y: fn_dho_forced(\n",
    "            t=t, x=y[0], v=y[1], gamma=gamma, omega=omega, f=f_val\n",
    "        ),\n",
    "        t_span=(start_t, end_t),\n",
    "        y0=ics,\n",
    "        t_eval=ts,\n",
    "    )\n",
    "    for omega in omegas\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "for res in results:\n",
    "    plt.plot(res.t, res.y[0])\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$x$\")\n",
    "\n",
    "ax.legend([r\"$\\omega$ = %.2f $\\omega_0$\" % sf for sf in omega_scaling_factors]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first note that the amplitudes are much greater for $0.95 \\omega_0$ than the others. The other difference is that as we increase $\\omega$, the frequency of oscillations gets higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Damping ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, looking at $\\gamma = 0.2, 0.5$, we will plot the amplitude of oscillations for different frequencies $\\omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma values to consider\n",
    "gammas = [0.2, 0.5]\n",
    "num_gammas = len(gammas)\n",
    "\n",
    "# Omegas to use\n",
    "omega0 = 2\n",
    "num_omegas = 500\n",
    "\n",
    "omega_scaling_factors = np.linspace(0.2, 2, num_omegas)\n",
    "omegas = omega0 * omega_scaling_factors\n",
    "\n",
    "# Time range - make sure the maximum amplitude will occur in this range\n",
    "start_t = 0\n",
    "end_t = 20\n",
    "\n",
    "# Initial conditions\n",
    "ics = [0, 0]\n",
    "\n",
    "# Other params\n",
    "f_val = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each gamma, find the amplitude for each omega\n",
    "data = np.zeros((num_gammas, num_omegas))\n",
    "\n",
    "for row, gamma in enumerate(gammas):\n",
    "    for col, omega in enumerate(omegas):\n",
    "        data[row][col] = (\n",
    "            integrate.solve_ivp(\n",
    "                fun=lambda t, y: fn_dho_forced(\n",
    "                    t=t, x=y[0], v=y[1], gamma=gamma, omega=omega, f=f_val\n",
    "                ),\n",
    "                t_span=(start_t, end_t),\n",
    "                y0=ics,\n",
    "            )\n",
    "            .y[0]\n",
    "            .max()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "fig, axes = plt.subplots(num_gammas, 1)\n",
    "fig.set_size_inches(10, 15)\n",
    "\n",
    "# Add titles\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.title.set_text(r\"$\\gamma = %.2f$\" % gammas[idx])\n",
    "\n",
    "# Add labels\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(r\"$\\omega_0$\")\n",
    "    ax.set_ylabel(\"amplitude\")\n",
    "\n",
    "# Adjust limits\n",
    "for ax in axes:\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "# Plot\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.plot(omega_scaling_factors, data[idx])\n",
    "    ax.grid(alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that in both cases the general shape of the plots is the same: they start off descending, then they have a \"bump\", then they decrease again (and, although it is not plotted, it appears as if they decay to zero). The differences in the two plots is mainly that in the case for the smaller $\\gamma$, the bump is more pronounced, while it is much less pronounced for the larger $\\gamma$ value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
