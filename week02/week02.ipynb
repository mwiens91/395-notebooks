{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.integrate as integrate\n",
    "from scipy.special import fresnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS 395 - week 2\n",
    "\n",
    "**Matt Wiens - #301294492**\n",
    "\n",
    "This notebook will be organized similarly to the lab script, with major headings corresponding to the headings on the lab script.\n",
    "\n",
    "*The TA's name (Ignacio) will be shortened to \"IC\" whenever used.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical error homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Truncation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncation error is the error involved in approximating an infinite sum by a finite sum. For example, the truncation error involved in approximating $e^x$ to the first three terms of its Taylor series is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left| \\, e^x - \\left(1 + x + \\frac{x^2}{2}\\right) \\, \\right|\n",
    "    .\n",
    "\\end{equation}\n",
    "\n",
    "For numerical integration, truncation error comes in to play because we are approximating an integral (an infinite sum) by a finite sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rounding error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any algorithm which produces a result, rounding error is the difference between producing that result using exact arithmetic and producing the result using finite-precision, rounded arithmetic (floating point arithmetic). On a computer, a number called \"machine epsilon\" gives you the upper bound of the relative error that occurs in floating point arithmetic due to rounding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demonstrating floating point arithmetic error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(7 / 3 - 4 / 3 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above in exact arithmetic should be $0$. But due to error in floating point arithmetic it is instead a non-zero small number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical integration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this section will be to investigate how we can approximate integrals according to different \"rules\".    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple rule\n",
    "\n",
    "Consider some function $f$ defined on an interval $[a, b]$. A simple rule to approximate the integral of $f$ over this interval is to take $N + 1$ evenly spaced points along the interval $\\{x_1, x_2, \\ldots, x_{N + 1}\\}$ and then add up the area of the rectangles induced by these points (see the picture in the lab script). Note that we always have $x_1 = a$ and $x_N = b$. This gives us the following approximation:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\int_a^b f(x) dx \\approx \\sum_{i = 1}^N f(x_i) h\n",
    "    ,\n",
    "\\end{equation}\n",
    "\n",
    "where $h$ is the width of each rectangle given by\n",
    "\n",
    "\\begin{equation}\n",
    "    h = \\frac{b - a}{N}\n",
    "    .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_left_reimann(y: np.ndarray, h: float) -> float:\n",
    "    \"\"\"Approximates an integral using the left Reimann sum.\"\"\"\n",
    "    return np.sum(y * h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will approximate the integral of $\\sin(x)$ over $[0, \\frac{\\pi}{2}]$ using logarithmically spaced values of $N$. We will plot the absolute error as a function of the bin width $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a number of N values and the corresponding h values\n",
    "num_ns = 50\n",
    "\n",
    "# We need to be a little careful since we want the N values to\n",
    "# be logarithmically space, but we also require that each N is an integer.\n",
    "n_vals = np.round(np.logspace(1, 6, num_ns)).astype(int)\n",
    "h_vals = np.pi / (2 * n_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we plot the absolute errors, note that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\int_0^{\\frac{\\pi}{2}} \\sin(x) dx = 1\n",
    "    .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each N value calculate the absolute error\n",
    "errors = np.zeros(num_ns)\n",
    "\n",
    "for idx, n in enumerate(n_vals):\n",
    "    xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "    ys = np.sin(xs)\n",
    "\n",
    "    errors[idx] = abs(1 - integrate_left_reimann(ys, np.pi / (2 * n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(h_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$h$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a linear relationship in the log-log plot. This means that the error E is related to $h$ through some relationship of the form\n",
    "\n",
    "\\begin{equation}\n",
    "    E = A h^\\alpha\n",
    "    .\n",
    "\\end{equation}\n",
    "\n",
    "This is because a linear relationship in log-log can be expressed as\n",
    "\n",
    "\\begin{align}\n",
    "    &\\log E = \\alpha \\log h + \\log A \\\\\n",
    "    &\\iff \\log E = \\log \\left( A h^\\alpha \\right) \\\\\n",
    "    &\\iff E = A h^\\alpha\n",
    "    .\n",
    "\\end{align}\n",
    "\n",
    "By inspection, I would guess that $\\alpha = 1$ and $A = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to fit the log values we obtained to a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the linear coefficients for the loglog relationship\n",
    "coeffs = np.polyfit(x=np.log(h_vals), y=np.log(errors), deg=1)\n",
    "\n",
    "# Translate to A and h\n",
    "print(\"A = %.2f\" % np.exp(coeffs[1]))\n",
    "print(\"alpha = %.2f\" % coeffs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My guess for $\\alpha$ was correct, but the value for $A$ I guessed was not quite right (it's difficult to make out on the above plot what $A$ should be with any precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the coefficients we calculated, we can estimate that to get an error less than or equal to $10^{-8}$ we would need a width of \n",
    "\n",
    "\\begin{align}\n",
    "    h &= \\left( \\frac{E}{A} \\right)^{\\frac{1}{\\alpha}} \\\\\n",
    "      &\\approx \\frac{10^{-8}}{0.49} \\\\\n",
    "      &\\approx 2 \\cdot 10^{-8}\n",
    "      ,\n",
    "\\end{align}\n",
    "\n",
    "which means we need at least $77712216$ slices using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting calculations for above text block\n",
    "needed_h = 1e-8 / np.exp(coeffs[1])\n",
    "needed_n = np.pi / 2 / needed_h\n",
    "\n",
    "print(\"approx h required: %e\" % needed_h)\n",
    "print(\"approx N required: %f\" % needed_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trapezoid rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the lab script we are shown a picture of how the trapezoid rule works. Given $x$ values $\\{x_0, x_1, x_2, x_3\\}$ and corresponding function values $\\{y_0, y_1, y_2, y_3\\}$. There are many ways to compute the sum of the trapezoids: for each trapezoid I will add the the rectangle induced the height of the right endpoint together with the remaining half rectangle. This gives us the formula for the sum $S$ as\n",
    "\n",
    "\\begin{align}\n",
    "    S &= \\sum_{i = 1}^3\n",
    "        \\left(\n",
    "            y_i h\n",
    "            + \\frac{1}{2} \\left( y_{i - 1} - y_i \\right) h\n",
    "        \\right) \\\\\n",
    "      &= \\frac{h}{2} \\sum_{i = 1}^3\n",
    "        \\left(\n",
    "            y_{i - 1} + y_i\n",
    "        \\right)\n",
    "\\end{align}\n",
    "\n",
    "where $h = x_i - x_{i - 1}$ (assumed constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the trapezoid rule, we will repeat the investigation we performed when using the left Riemann approximation. We will keep the same number of $N$ values to test and the corresponding $h$ values we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each N value calculate the absolute error\n",
    "errors = np.zeros(num_ns)\n",
    "\n",
    "for idx, n in enumerate(n_vals):\n",
    "    xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "    ys = np.sin(xs)\n",
    "\n",
    "    errors[idx] = abs(1 - integrate.trapz(y=ys, dx=np.pi / (2 * n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(h_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$h$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the log-log relationship is still linear. However, for the same values of $h$ the error in the trapezoidal rule is much lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate the $A$ and $\\alpha$ parameters and estimate what $h$ and $N$ we would need to keep the absolute error below $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the linear coefficients for the loglog relationship\n",
    "coeffs = np.polyfit(x=np.log(h_vals), y=np.log(errors), deg=1)\n",
    "\n",
    "# Translate to A and h\n",
    "print(\"A = %.2f\" % np.exp(coeffs[1]))\n",
    "print(\"alpha = %.2f\" % coeffs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the greatest h/ least N we can use?\n",
    "needed_h = 1e-8 / np.exp(coeffs[1])\n",
    "needed_n = np.pi / 2 / needed_h\n",
    "\n",
    "print(\"approx h required: %e\" % needed_h)\n",
    "print(\"approx N required: %f\" % needed_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare with the left Riemann approximation, we need $77712216 - 13088578 = 64623638$ less steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpson's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for Simpson's method we need an *even* number of slices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll plot the errors for different values of $h$. Note that we need to be careful to make sure $N$ is even here, so we'll adjust each $N$ value so that this holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to be a little careful since we want the N values to\n",
    "# be logarithmically space, but we also require that each N is an integer.\n",
    "simps_n_vals = n_vals + n_vals % 2\n",
    "simps_h_vals = np.pi / (2 * n_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each N value calculate the absolute error\n",
    "errors = np.zeros(num_ns)\n",
    "\n",
    "for idx, n in enumerate(simps_n_vals):\n",
    "    xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "    ys = np.sin(xs)\n",
    "\n",
    "    errors[idx] = abs(1 - integrate.simps(y=ys, dx=np.pi / (2 * n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(simps_h_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$h$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly see that Simpson's method outperforms both methods we looked at before. We quickly reach the limits of machine precision as we increase $N$. The dependence is still linear, although due to the limits of machine precision, we cannot show the full relationship we would get with exact arithmetic here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If wanted to fit the linear relationship we need to look at the values of $N$ that lead to $h \\approx 10^{-3}$ and higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last h value index to keep\n",
    "last_h_idx = 20\n",
    "\n",
    "# Find the linear coefficients for the loglog relationship\n",
    "coeffs = np.polyfit(\n",
    "    x=np.log(simps_h_vals[: last_h_idx + 1]), y=np.log(errors[: last_h_idx + 1]), deg=1\n",
    ")\n",
    "\n",
    "# Translate to A and h\n",
    "print(\"A = %.2f\" % np.exp(coeffs[1]))\n",
    "print(\"alpha = %.2f\" % coeffs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can again determine what is the greatest $H$/least $N$ we need to keep the error under $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the greatest h/ least N we can use?\n",
    "needed_h = 1e-8 / np.exp(coeffs[1])\n",
    "needed_n = np.pi / 2 / needed_h\n",
    "\n",
    "print(\"approx h required: %e\" % needed_h)\n",
    "print(\"approx N required: %f\" % needed_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the $A$ value or by looking that the greatest $N$ required, Simpson's method is about twice as efficient as the trapezoid rule  and about eight times as efficient as the left Riemann sum approximation for this integral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing comparison\n",
    "\n",
    "Let's compare how long each of these methods take. While we know Simpson's method uses less steps, for example, each step could take longer, so up to now, we still don't know which method is \"best\". We'll now time each method using the $N$ value for each method which gives error below $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N values\n",
    "test_n_vals = n_vals + n_vals % 2\n",
    "test_h_vals = np.pi / (2 * n_vals)\n",
    "\n",
    "# Left Riemann\n",
    "n = 77712216\n",
    "xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "ys = np.sin(xs)\n",
    "h = np.pi / (2 * n)\n",
    "\n",
    "start = timer()\n",
    "\n",
    "integrate_left_reimann(y=ys, h=h)\n",
    "\n",
    "lriem_time = timer() - start\n",
    "\n",
    "# Trapezoid\n",
    "n = 13088578\n",
    "xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "ys = np.sin(xs)\n",
    "h = np.pi / (2 * n)\n",
    "\n",
    "start = timer()\n",
    "\n",
    "integrate.simps(y=ys, dx=h)\n",
    "\n",
    "trapz_time = timer() - start\n",
    "\n",
    "# Simpson's\n",
    "n = 799272\n",
    "xs = np.linspace(0, np.pi / 2, n + 1)\n",
    "ys = np.sin(xs)\n",
    "h = np.pi / (2 * n)\n",
    "\n",
    "start = timer()\n",
    "\n",
    "integrate.simps(y=ys, dx=h)\n",
    "\n",
    "simps_time = timer() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the times\n",
    "print(\"Left Riemann:\\t%.5f\" % lriem_time)\n",
    "print(\"Trapezoid:\\t%.5f\" % trapz_time)\n",
    "print(\"Simpson's:\\t%.5f\" % simps_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least for this integral, Simpson's method is by bar the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1 homework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fresnel integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fresnel integral is given by the following function:\n",
    "\n",
    "\\begin{equation}\n",
    "    C(x) = \\int_0^x \\cos \\left( \\frac{\\pi}{2} x^2 \\right) dx\n",
    "    .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first plot the integrand on the range $[0, 7.1]$.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 7.1, 500)\n",
    "ys = np.cos(np.pi / 2 * xs ** 2)\n",
    "\n",
    "# Plot\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.plot(xs, ys)\n",
    "\n",
    "ax.set_xlabel(r\"x\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SciPy, we can evaluate $C$ at $x = 7.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, c_val = fresnel(7.1)\n",
    "\n",
    "print(c_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Simpson's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used the numerical integration methods we discussed above, however, we would need a very large number of slices $N$. This is because, as we can see in the above plot, the integrand of the Fresnel function changes rapidly, which means that any approximation per slice, especially as $x$ gets large, is going to be bad. In this case, it might make more sense to have less slices near the beginning of the interval (when the integrand doesn't change as rapidly), and more slices as we progress through the interval, since the integrand is changing more and more with increasing $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find what $h$ and $N$ we need using Simpson's method to keep the error less then $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each N value calculate the absolute error\n",
    "simps_n_vals = n_vals + n_vals % 2\n",
    "simps_h_vals = 7.1 / simps_n_vals\n",
    "\n",
    "errors = np.zeros(num_ns)\n",
    "\n",
    "for idx, n in enumerate(simps_n_vals):\n",
    "    xs = np.linspace(0, 7.1, n + 1)\n",
    "    ys = np.cos(np.pi / 2 * xs ** 2)\n",
    "\n",
    "    errors[idx] = abs(c_val - integrate.simps(y=ys, dx=simps_h_vals[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(simps_h_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$h$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll determine the coefficients of the loglog relationship using the $h$ values that appear linear on the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which h indices we'll use\n",
    "first_h_idx = 8\n",
    "last_h_idx = 35\n",
    "\n",
    "# Find the linear coefficients for the loglog relationship\n",
    "coeffs = np.polyfit(\n",
    "    x=np.log(simps_h_vals[first_h_idx : last_h_idx + 1]),\n",
    "    y=np.log(errors[first_h_idx : last_h_idx + 1]),\n",
    "    deg=1,\n",
    ")\n",
    "\n",
    "# Translate to A and h\n",
    "print(\"A = %.2f\" % np.exp(coeffs[1]))\n",
    "print(\"alpha = %.2f\" % coeffs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that determining these coefficients is more sensitive to the data points chosen compared to the cases we looked at earlier. Nevertheless, the coefficients are relatively stable at the above values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the greatest h/ least N we can use?\n",
    "needed_h = 1e-8 / np.exp(coeffs[1])\n",
    "needed_n = np.pi / 2 / needed_h\n",
    "\n",
    "print(\"approx h required: %e\" % needed_h)\n",
    "print(\"approx N required: %f\" % needed_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of steps $N$ is around three orders of magnitude higher than what we needed for the simpler integral we first looked at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using non-uniformly spaces points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine what $N$ we need using SciPy's implementation of fixed-order Gaussian quadrature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The integrand\n",
    "integrand = lambda x: np.cos(np.pi / 2 * x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the errors. We'll use less N values here because it can\n",
    "# take quite awhile using the N values we've been using so far.\n",
    "quad_num_ns = 50\n",
    "quad_n_vals = np.round(np.logspace(1, 3.5, quad_num_ns)).astype(int)\n",
    "errors = np.zeros(quad_num_ns)\n",
    "\n",
    "for idx, n in enumerate(quad_n_vals):\n",
    "    errors[idx] = abs(c_val - integrate.fixed_quad(func=integrand, a=0, b=7.1, n=n)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors against N\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(quad_n_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$N$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the above plot, it looks like we can use around $N \\approx 45$ to reach an absolute error below $10^{-8}$. Let's verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error with N = 90\n",
    "this_error = abs(c_val - integrate.fixed_quad(func=integrand, a=0, b=7.1, n=45)[0])\n",
    "\n",
    "print(\"error with N=%d: %e\" % (45, this_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy also provides another function which lets us specify the tolerance specifically. Let's try it with a tolerance of $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"value:\\t%e\\nerror:\\t%e\"\n",
    "    % integrate.quadrature(func=integrand, a=0, b=7.1, tol=1e-8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating improper integrals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a SciPy quadrature function to evaluate improper integrals like\n",
    "\n",
    "\\begin{equation}\n",
    "    \\int_{-\\infty}^\\infty \\exp \\left(- \\frac{x^2}{2} \\right) dx = \\sqrt{2 \\pi}\n",
    "    .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"value:\\t%e\\nerror:\\t%e\"\n",
    "    % integrate.quad(func=lambda x: np.exp(-(x ** 2) / 2), a=-np.inf, b=np.inf)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll revisit the Fresnel integral but this time using Monte Carlo integration. We'll keep using the $N$ values in our `n_vals` array; note however that the interpretation behind $N$ is different: before $N$ was the number of slices and now it is the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors\n",
    "errors = np.zeros(num_ns)\n",
    "\n",
    "for idx, n in enumerate(n_vals):\n",
    "    xs = np.random.uniform(0, 7.1, n)\n",
    "    ys = np.cos(np.pi / 2 * xs ** 2)\n",
    "\n",
    "    est = 7.1 * np.sum(ys) / n\n",
    "\n",
    "    errors[idx] = abs(c_val - est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors against N\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plt.loglog(n_vals, errors, \"o\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(r\"$N$\")\n",
    "ax.set_ylabel(\"abs error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For estimating the Fresnal integral, Monte Carlo seems like a poor choice compared to the other options we've investigated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
