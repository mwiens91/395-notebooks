{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import (\n",
    "    Activation,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    MaxPooling2D,\n",
    ")\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS 395 - week 9\n",
    "\n",
    "**Matt Wiens - #301294492**\n",
    "\n",
    "This notebook will be organized similarly to the lab script, with major headings corresponding to the headings on the lab script.\n",
    "\n",
    "*The TA's name (Ignacio) will be shortened to \"IC\" whenever used.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural nets (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're going to be training a model to recognize when Ising model-like data is at configurations above, at, or below critical temperatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll read in Ising model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lattice size\n",
    "L = 32\n",
    "\n",
    "# Critical temp\n",
    "T_c = 2.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory relative paths\n",
    "data_dir = \"Ising\"\n",
    "data_sub_dirs = [\n",
    "    \"T1.5\",\n",
    "    \"T2.0\",\n",
    "    \"T2.1\",\n",
    "    \"T2.2\",\n",
    "    \"T2.25\",\n",
    "    \"T2.26\",\n",
    "    \"T2.27\",\n",
    "    \"T2.28\",\n",
    "    \"T2.29\",\n",
    "    \"T2.3\",\n",
    "    \"T2.4\",\n",
    "    \"T2.5\",\n",
    "    \"T2.7\",\n",
    "    \"T3.0\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all Ising model data\n",
    "X = []  # images\n",
    "Y = []  # labels\n",
    "temps = []  # temperatures\n",
    "\n",
    "for sub_dir in data_sub_dirs:\n",
    "    temp = float(sub_dir[1:])\n",
    "    label = 1 if temp <= T_c else 0\n",
    "\n",
    "    for i in range(1, 501):\n",
    "        # Read in data\n",
    "        x = np.genfromtxt(os.path.join(data_dir, sub_dir, \"img\" + str(i) + \".dat\"))\n",
    "        x = np.reshape(x, (L, L, 1))\n",
    "\n",
    "        # Store data\n",
    "        X.append(x)\n",
    "        Y.append(label)\n",
    "        temps.append(temp)\n",
    "\n",
    "# Convert X, Y, and temperature lists into arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "temps = np.array(temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the size of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape: %s\" % list(X.shape))\n",
    "print(\"Y shape: %s\" % list(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's have a look at some of the images in each of the following regimes: below, near, or above $T_c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_idxs = [50, 550, 1050]\n",
    "near_idxs = [3001, 3102, 3403]\n",
    "above_idxs = [5405, 6110, 6991]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low temperatures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 3, figsize=[20, 20 / 3])\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    i = low_idxs[idx]\n",
    "\n",
    "    ax.imshow(X[i, :, :, 0])\n",
    "    ax.set_title(\"T = %s\" % temps[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see pretty clearly that at very low temperature, essentially all of the spins are aligned. As we increase temperature, we get small deviations from the majority of aligned spins. This deviations get larger the higher the temperature in this regime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Near critical temperature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 3, figsize=[20, 20 / 3])\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    i = near_idxs[idx]\n",
    "\n",
    "    ax.imshow(X[i, :, :, 0])\n",
    "    ax.set_title(\"T = %s\" % temps[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a similar trend as before at the critical temperature. At appears that the majority of spins are aligned, while some, but a significant amount, are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above critical temperature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 3, figsize=[20, 20 / 3])\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    i = above_idxs[idx]\n",
    "\n",
    "    ax.imshow(X[i, :, :, 0])\n",
    "    ax.set_title(\"T = %s\" % temps[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it seems that the spins have close to a net zero sum, which seems to hold better the higher we increase the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating data into training, test, and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll split our data using a 70/15/15 distribution of our data into training, test, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "\n",
    "N_train = int(N * 0.70)\n",
    "N_test = int(N * 0.15)\n",
    "N_valid = int(N * 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle indices\n",
    "idxs = np.random.permutation(N)\n",
    "\n",
    "idxs_train = idxs[:N_train]\n",
    "idxs_test = idxs[N_train : N_train + N_test]\n",
    "idxs_valid = idxs[N_train + N_test :]\n",
    "\n",
    "# Now partition our data using these indices\n",
    "# into training, test, and validation sets\n",
    "X_train = X[idxs_train, :, :, :]\n",
    "Y_train = Y[idxs_train]\n",
    "temps_train = temps[idxs_train]\n",
    "\n",
    "X_test = X[idxs_test, :, :, :]\n",
    "Y_test = Y[idxs_test]\n",
    "temps_test = temps[idxs_test]\n",
    "\n",
    "X_valid = X[idxs_valid, :, :, :]\n",
    "Y_valid = Y[idxs_valid]\n",
    "temps_valid = temps[idxs_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make use of the fact that our Ising configurations are symmetric to generate more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag indicating whether to add augmented data\n",
    "use_augmented_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_augmented_data:\n",
    "    X_data_to_add = np.zeros((2 * N_train, L, L, 1))\n",
    "    Y_data_to_add = np.zeros(2 * N_train)\n",
    "    temp_data_to_add = np.zeros(2 * N_train)\n",
    "\n",
    "    for i, (x, y, temp) in enumerate(zip(X_train, Y_train, temps_train)):\n",
    "        X_data_to_add[2 * i, :, :, :] = X_train[i, :, ::-1, :]\n",
    "        X_data_to_add[2 * i + 1, :, :, :] = X_train[i, ::-1, :, :]\n",
    "\n",
    "        Y_data_to_add[2 * i] = y\n",
    "        Y_data_to_add[2 * i + 1] = y\n",
    "\n",
    "        temp_data_to_add[2 * i] = temp\n",
    "        temp_data_to_add[2 * i + 1] = temp\n",
    "        \n",
    "    X_train = np.concatenate((X_train, X_data_to_add))\n",
    "    Y_train = np.concatenate((Y_train, Y_data_to_add))\n",
    "    temps_train = np.concatenate((temps_train, temp_data_to_add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll set up a DNN using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(L, L, 1))\n",
    "\n",
    "x = Flatten()(inputs)\n",
    "\n",
    "x = Dense(512)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "x = Dense(256)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "x = Dense(1)(x)\n",
    "\n",
    "outputs = Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "sgd = optimizers.SGD(lr=lr)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show a summary of the model we compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have ~650,000 trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train the model with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    validation_data=[X_valid, Y_valid],\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the model on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, val in zip(model.metrics_names, vals):\n",
    "    print(\"%s: %.2f\" % (name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the average prediction values as a function of temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pY = pd.DataFrame({\"pY\": pY[:, 0], \"T\": temps_test})\n",
    "df_pY_avg = df_pY.groupby(\"T\", as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df_pY_avg.T.values[0, :], df_pY_avg.T.values[1, :], \"*-\")\n",
    "\n",
    "ax.set_xlabel(\"T\")\n",
    "ax.set_ylabel(r\"$p_Y$\")\n",
    "ax.set_ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting region of this plot (where there is the most uncertainty) definitely seems to correspond to where the critical temperature is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust the `lr` value in our model from `0.01` to `0.001` as suggested in the lab script. The lab script suggests that this would lead to worse performance but better accuracy. I ran both of these settings a few time and honestly (1) found the performance impact negligible, (2) didn't notice a significant difference in accuracy.\n",
    "\n",
    "Probably lowering the `lr` value even further would make a difference I would notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding an extra layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see the effect of adding in another layer. We can do this by adding the following code to our model code (right before `x = Dense(1)(x)`):\n",
    "\n",
    "```\n",
    "x = Dense(512)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(dropout)(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my runs, accuracy did *not* improve (at least I didn't notice an improvement) from adding an extra later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting the dropout values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try lowering the dropout values. Using values of `0.1` and `0.3`, it appeared that the greater the number of iterations done during fitting. However, the accuracy still didn't substantially increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can augment our training data by making use of symmetries in the Ising model. Code for this is above, you just need to rerun the notebook and set `use_augmented_data` to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran through several runs of this, and on average I saw a ~2% increase in accuracy, which is a lot more than I saw for either of the other methods is above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
